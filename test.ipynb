{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Extraction from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FloatObject (b'0.00-16291952') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-17076502') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-18528813') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-16291952') invalid; use 0.0 instead\n",
      "FloatObject (b'0.00-16291952') invalid; use 0.0 instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total valid paragraphs: 1337\n",
      "(Page 1) Towards characterizing dark matter subhalo perturbations in stellar streams with graph neural\n",
      "\n",
      "(Page 1) 1Department of Astronomy, UC Berkeley, 501 Campbell Hall, Berkeley, CA, 94720, United States of America\n",
      "\n",
      "(Page 1) 2Department of Mathematics, University of Toronto, 40 St. George Street, Toronto, ON, M5S 2E4, Canada\n",
      "\n",
      "(Page 1) 3Department of Physics, Imperial College London, Blackett Laboratory, Prince Consort Road, London, SW7 2AZ, United Kingdom\n",
      "\n",
      "(Page 1) 4Dunlap Institute for Astronomy and Astrophysics, University of Toronto, 50 St. George Street, Toronto, ON, M5S 3H4, Canada\n",
      "\n",
      "(Page 1) 5David A. Dunlap Department of Astronomy and Astrophysics, University of Toronto,\n",
      "\n",
      "(Page 1) 50 St. George Street, Toronto, ON, M5S 3H4, Canada\n",
      "\n",
      "(Page 1) 6Department of Science, Technology and Society, Division of Natural Science, York University,\n",
      "\n",
      "(Page 1) 218 Bethune College, Toronto, ON, M3J 1P3, Canada\n",
      "\n",
      "(Page 1) The phase space of stellar streams is proposed to detect dark substructure in the Milky Way through\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "# Define the minimum word count for a paragraph to be considered valid\n",
    "MIN_WORD_COUNT = 8  \n",
    "\n",
    "# Open the PDF file in read-binary mode\n",
    "with open('data/paper.pdf', 'rb') as file:\n",
    "    # Create a PDF reader object\n",
    "    pdf_reader = PyPDF2.PdfReader(file)\n",
    "    \n",
    "    # Get the number of pages\n",
    "    num_pages = len(pdf_reader.pages)\n",
    "    \n",
    "    # List to store extracted paragraphs with page numbers\n",
    "    paragraphs_with_pages = []\n",
    "\n",
    "    # Loop through each page and extract text\n",
    "    for page_num in range(num_pages):\n",
    "        page = pdf_reader.pages[page_num]\n",
    "        text = page.extract_text()\n",
    "        \n",
    "        if text:  # Ensure the page contains text\n",
    "            # Split text into paragraphs based on newlines\n",
    "            paragraphs = text.split(\"\\n\")\n",
    "            \n",
    "            # Clean up empty lines and unwanted characters\n",
    "            paragraphs = [p.strip() for p in paragraphs if p.strip()]\n",
    "            \n",
    "            # Filter out short paragraphs\n",
    "            valid_paragraphs = [p for p in paragraphs if len(p.split()) >= MIN_WORD_COUNT]\n",
    "            \n",
    "            # Store paragraphs with the associated page number\n",
    "            for paragraph in valid_paragraphs:\n",
    "                paragraphs_with_pages.append((paragraph, page_num + 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separation of extracted text in paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering by length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(url=\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'texts = [\"king\",\\n         \"queen\",\\n         \"dictator\",\\n         \"hitler\",\\n         \"Berlin is the German capital city and is near Austria, Hitler\\'s birthplace\",\\n         \"austria\",\\n         \"Hitler was born in the city of Vienna\",\\n         \"The city of Vienna is in Austria\"\\n]'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"texts = [\"king\",\n",
    "         \"queen\",\n",
    "         \"dictator\",\n",
    "         \"hitler\",\n",
    "         \"Berlin is the German capital city and is near Austria, Hitler's birthplace\",\n",
    "         \"austria\",\n",
    "         \"Hitler was born in the city of Vienna\",\n",
    "         \"The city of Vienna is in Austria\"\n",
    "]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5189, 384)\n"
     ]
    }
   ],
   "source": [
    "embeddings = embedding_model.encode(paragraphs)\n",
    "print(embeddings.shape)  # (2, 384)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings import in database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1337\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "points = [\n",
    "    PointStruct(\n",
    "        id=idx,\n",
    "        vector=vector,\n",
    "        payload={\n",
    "            \"text\": text,\n",
    "            \"page\": num_pag\n",
    "        },\n",
    "    )\n",
    "    for idx, (vector, (text, num_pag)) in enumerate(zip(embeddings, paragraphs_with_pages))\n",
    "    \n",
    "]\n",
    "\n",
    "print(len(points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client.models import VectorParams, Distance\n",
    "\n",
    "collection_name = \"pdf_embeddings\"\n",
    "\n",
    "client.delete_collection(collection_name)\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name,\n",
    "    vectors_config=VectorParams(\n",
    "        size=384,\n",
    "        distance=Distance.COSINE,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading embeddings to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of batching points\n",
    "batch_size = 1000  # Adjust as needed\n",
    "for i in range(0, len(points), batch_size):\n",
    "    batch = points[i:i + batch_size]\n",
    "    client.upsert(collection_name, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.query_points(\n",
    "    collection_name=collection_name,\n",
    "    query=embedding_model.encode(\"asd?\"),\n",
    "    limit=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313\n",
      "5\n",
      "318\n",
      "71\n",
      "305\n",
      "187\n",
      "498\n",
      "168\n",
      "156\n",
      "228\n"
     ]
    }
   ],
   "source": [
    "for point in result.points:\n",
    "    print(point.payload[\"page\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryResponse(points=[ScoredPoint(id=312, version=0, score=0.37698925, payload={'text': 'sequentially as they are in ASM. The Resource', 'page': 313}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=4, version=0, score=0.34734514, payload={'text': 'This book is provided “as -is” and expresses the', 'page': 5}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=317, version=0, score=0.33376485, payload={'text': 'the ASM model, all services had to be updated at', 'page': 318}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=70, version=0, score=0.33192196, payload={'text': 'but also for those who need a refresher a nd', 'page': 71}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=304, version=0, score=0.26356944, payload={'text': 'the same resource group and manage a nd', 'page': 305}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=186, version=0, score=0.25505117, payload={'text': 'software services you require on an as -needed', 'page': 187}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=497, version=0, score=0.21722609, payload={'text': 'are the Actions you would assign t o that role:  31 of 540 C H A P T E R  1  |  Getting  started with Microsoft Azure', 'page': 498}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=167, version=0, score=0.21657288, payload={'text': 'If you ne ed additional su pport, e mail Microsoft', 'page': 168}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=155, version=0, score=0.20916411, payload={'text': 'Ramabathiran from the Azur e App Service team', 'page': 156}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=227, version=0, score=0.19934532, payload={'text': 'You would not face a large up -front capital', 'page': 228}, vector=None, shard_key=None, order_value=None)])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
